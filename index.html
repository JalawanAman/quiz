<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>DS MCQ Practice</title>


<style>

    .header h1 {
        color: #ff4444;
        font-size: 2.5em;
        margin-bottom: 10px;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
    }

    .header p {
        color: #ffaaaa;
        font-size: 1.1em;
    }




* { margin: 0; padding: 0; box-sizing: border-box; }

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background: linear-gradient(135deg, #1a0000, #2d0a0a, #1a0000);
    color: #e0e0e0;
    min-height: 100vh;
    padding: 20px;
}

.container { max-width: 900px; margin: auto; }

/* HEADER */
    .header {
        text-align: center;
        padding: 25px;
        background: rgba(139,0,0,0.2);
        border-radius: 15px;
        border: 2px solid #8b0000;
        margin-bottom: 20px;
    }


/* PROGRESS */
.progress-bar {
    height: 26px;
    background: rgba(0,0,0,0.4);
    border-radius: 13px;
    overflow: hidden;
    border: 2px solid #8b0000;
    margin-bottom: 15px;
}
.progress-fill {
    height: 100%;
    background: linear-gradient(90deg, #8b0000, #ff4444);
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: bold;
}

/* RESULT STRIP (AFTER REVIEW) */
.result-strip {
    display: none;
    margin-bottom: 15px;
    padding: 12px;
    background: rgba(0,0,0,0.45);
    border: 2px solid #8b0000;
    border-radius: 12px;
    text-align: center;
    font-weight: bold;
}
.result-strip span { margin: 0 10px; color: #ffaaaa; }

/* QUESTION CARD */
.question-card {
    background: rgba(0,0,0,0.5);
    border: 2px solid #8b0000;
    border-radius: 15px;
    padding: 20px;
    margin-bottom: 15px;
}
.question-number {
    color: #ff4444;
    font-weight: bold;
    margin-bottom: 10px;
}
.question-text {
    font-size: 1.15em;
    margin-bottom: 20px;
}

/* EXPLANATION BOX */
.explanation-box {
    margin-top: 15px;
    padding: 14px 16px;
    border-radius: 12px;
    font-size: 0.95em;
    line-height: 1.5;
    backdrop-filter: blur(6px);
}

.explanation-correct {
    background: rgba(52, 152, 219, 0.18);
    border: 2px solid rgba(52, 152, 219, 0.6);
    color: #cfe9ff;
}

.explanation-wrong {
    background: rgba(243, 156, 18, 0.18);
    border: 2px solid rgba(243, 156, 18, 0.6);
    color: #ffe4b5;
}

.explanation-title {
    font-weight: bold;
    margin-bottom: 6px;
}



/* OPTIONS */
.options { display: flex; flex-direction: column; gap: 12px; }

.option {
    background: rgba(30,0,0,0.6);
    border: 2px solid #5a0000;
    border-radius: 10px;
    padding: 12px 16px;
    cursor: pointer;
    display: flex;
    gap: 12px;
    transition: all 0.2s ease;
}
.option.selected {
    border-color: #ff4444;
    background: rgba(139,0,0,0.45);
}
.option.correct {
    background: rgba(46,204,113,0.15);
    border-color: rgba(46,204,113,0.8);
    box-shadow: 0 0 10px rgba(46,204,113,0.35);
}
.option.incorrect {
    background: rgba(231,76,60,0.15);
    border-color: rgba(231,76,60,0.8);
    box-shadow: 0 0 10px rgba(231,76,60,0.35);
}
.option-label {
    width: 32px;
    height: 32px;
    border-radius: 50%;
    background: #8b0000;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: bold;
}

.question-navigator {
    margin-top: 26px;      /* THIS is the missing separation */
}


/* NAVIGATOR (BELOW MCQ) */
.question-navigator {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    padding: 12px;
    background: rgba(0,0,0,0.35);
    border: 2px solid #8b0000;
    border-radius: 12px;

    margin-top: 26px;      /* gap from MCQ + explanation */
    margin-bottom: 90px;  /* space above sticky nav */
}

.q-nav-item {
    width: 36px;
    height: 36px;
    background: rgba(139,0,0,0.25);
    border: 1px solid #8b0000;
    border-radius: 6px;
    font-size: 0.8em;
    color: #ffaaaa;
    display: flex;
    align-items: center;
    justify-content: center;
    cursor: pointer;
}
.q-nav-item.answered { background: rgba(0,100,0,0.35); }
.q-nav-item.current { background: #ff4444; color: #fff; }

/* NAVIGATION BAR */
.navigation {
    position: sticky;
    bottom: 0;
    background: rgba(0,0,0,0.85);
    padding: 12px;
    border-top: 2px solid #8b0000;
    display: flex;
    justify-content: center;
    gap: 15px;
}
.btn {
    padding: 10px 26px;
    border-radius: 8px;
    border: none;
    font-weight: bold;
    cursor: pointer;
}
.btn-primary {
    background: linear-gradient(135deg,#8b0000,#ff4444);
    color: white;
}
.btn-secondary {
    background: rgba(139,0,0,0.3);
    color: #ff4444;
    border: 2px solid #8b0000;
}

/* RESULTS MODAL (OLD STYLE) */
.results {
    position: fixed;
    inset: 0;
    background: rgba(0,0,0,0.75);
    display: none;
    align-items: center;
    justify-content: center;
    z-index: 1000;
}
.results .box {
    background: rgba(0,0,0,0.6);
    border: 2px solid #8b0000;
    border-radius: 15px;
    padding: 40px;
    text-align: center;
}
.score-display {
    font-size: 4em;
    color: #ff4444;
    margin: 20px 0;
}
.stats {
    display: grid;
    grid-template-columns: repeat(2,1fr);
    gap: 15px;
}
.stat-card {
    background: rgba(139,0,0,0.2);
    padding: 15px;
    border-radius: 10px;
}
.stat-value { font-size: 1.8em; color: #ff4444; }
.stat-label { color: #ffaaaa; }
</style>
</head>

<body>
<div class="container">

<div class="header">
    <h1>Intro to DS MCQ Test</h1>
    <p>:)</p>
</div>

<div class="progress-bar">
    <div class="progress-fill" id="progressBar">0/0</div>
</div>

<div class="result-strip" id="resultStrip"></div>

<div id="quizContainer"></div>
<div class="question-navigator" id="questionNavigator"></div>

<div class="navigation">
    <button class="btn btn-secondary" onclick="previousQuestion()">Previous</button>
    <button class="btn btn-primary" onclick="nextQuestion()">Next</button>
    <button class="btn btn-primary" onclick="submitTest()">Submit</button>
    <button class="btn btn-secondary" id="retakeBtn" onclick="location.reload()" style="display:none;">Retake</button>
</div>

</div>

<!-- OLD RESULT MODAL -->
<div class="results" id="results">
    <div class="box" id="resultsBox"></div>
</div>

<script>
/* ===== DATA (REPLACE THIS ONLY) ===== */
const rawQuestions = [
  {
    question: "What is the very first step in hierarchical clustering before any clusters are formed?",
    options: [
      "Decide the final number of clusters",
      "Merge the two closest clusters",
      "Create a distance matrix between all individual points",
      "Choose a linkage method and stop"
    ],
    correct: 2,
    why: "Hierarchical clustering always starts with all points as individual clusters. To know which ones are closest, we must first compute the distance between every pair of points. Merging happens only after this table exists, and the number of clusters is not fixed in advance."
  },
  {
    question: "In single linkage clustering, how is the distance between two clusters defined?",
    options: [
      "The average distance between all points in both clusters",
      "The maximum distance between any two points in the clusters",
      "The minimum distance between any point in one cluster and any point in the other",
      "The distance between the cluster centroids"
    ],
    correct: 2,
    why: "Single linkage measures cluster distance using the closest possible pair across the two clusters. The other options describe average linkage, complete linkage, or centroid-based methods, not single linkage."
  },
  {
    question: "Why does single linkage often create long, chain-like clusters?",
    options: [
      "Because it always uses maximum distances",
      "Because it merges clusters if there is any close connecting point",
      "Because it recalculates distances incorrectly",
      "Because it stops merging too early"
    ],
    correct: 1,
    why: "Single linkage only needs one close pair to merge clusters. This allows clusters to grow step by step through connecting points, even if the overall cluster becomes very spread out, creating the chaining effect."
  },
  {
    question: "In complete linkage, which value represents the distance between two clusters?",
    options: [
      "The smallest distance between any two points across clusters",
      "The distance between the closest centroids",
      "The largest distance between any two points across clusters",
      "The sum of all distances across clusters"
    ],
    correct: 2,
    why: "Complete linkage defines cluster distance using the farthest pair of points across the two clusters. This captures the worst-case separation and helps enforce compact clusters."
  },
  {
    question: "How does complete linkage decide which clusters to merge next?",
    options: [
      "It merges clusters with the largest maximum distance",
      "It merges clusters with the smallest average distance",
      "It merges clusters with the smallest of all maximum distances",
      "It merges clusters randomly to avoid chaining"
    ],
    correct: 2,
    why: "Complete linkage computes a maximum distance for each cluster pair, then selects the pair whose maximum distance is the smallest. This chooses the safest merge with the least worst-case separation."
  },
  {
    question: "A dataset forms a curved or snake-like shape in space. Which linkage method is more suitable to preserve this shape?",
    options: [
      "Complete linkage, because it avoids long clusters",
      "Single linkage, because it follows connectivity",
      "Complete linkage, because it minimizes maximum distance",
      "Centroid linkage, because it ignores individual points"
    ],
    correct: 1,
    why: "Single linkage is better for preserving connectivity and natural shapes. As long as points are locally close, single linkage keeps them connected, while complete linkage would split the shape due to far endpoints."
  },
  {
    question: "When would complete linkage be preferred over single linkage in practice?",
    options: [
      "When detecting paths or networks",
      "When clusters must be internally compact and similar",
      "When the data has a natural ordering",
      "When only one cluster is desired"
    ],
    correct: 1,
    why: "Complete linkage is chosen when compactness matters. It avoids merging clusters that would contain very distant points, unlike single linkage which prioritizes connectivity."
  },
  {
    question: "What remains the same across all distance tables during hierarchical clustering?",
    options: [
      "The numerical distance values",
      "The linkage rule being applied",
      "The labels of individual points only",
      "The number of clusters"
    ],
    correct: 1,
    why: "Once chosen, the linkage rule (single or complete) stays fixed throughout the algorithm. Distances, labels, and cluster counts change at each step."
  },
  {
    question: "After merging two points into a cluster, how are distances to other clusters computed?",
    options: [
      "By reusing the old point-to-point distances directly",
      "By ignoring distances involving merged points",
      "By applying the chosen linkage rule between clusters",
      "By recalculating distances from scratch using centroids"
    ],
    correct: 2,
    why: "Once clusters form, distances are computed between clusters using the selected linkage rule. Single linkage uses minimum distances, and complete linkage uses maximum distances."
  },
  {
    question: "Which statement best summarizes the core difference between single and complete linkage?",
    options: [
      "Single linkage uses rows, complete linkage uses columns",
      "Single linkage uses min of mins, complete linkage uses min of maxes",
      "Single linkage stops early, complete linkage stops late",
      "Single linkage is random, complete linkage is deterministic"
    ],
    correct: 1,
    why: "Single linkage merges based on the smallest minimum distance between clusters, while complete linkage merges based on the smallest maximum distance. This single change leads to very different clustering behavior."
  },
    {
    question: "Two clusters A and B each contain multiple points. In complete linkage, what exactly does the computed maximum distance represent?",
    options: [
      "The distance between the two farthest points inside cluster A",
      "The distance between the two farthest points inside cluster B",
      "The worst-case separation between any point in A and any point in B",
      "The distance between the centroids of A and B"
    ],
    correct: 2,
    why: "Complete linkage compares clusters by considering all cross-cluster point pairs. The maximum of these distances represents the worst-case separation that would exist if the clusters were merged. The other options focus on internal or centroid distances, which are not used in complete linkage."
  },
  {
    question: "Why does complete linkage tend to delay merges compared to single linkage on the same dataset?",
    options: [
      "Because it uses average distances instead of minimum distances",
      "Because it penalizes merges that would create large internal spread",
      "Because it recalculates the distance matrix incorrectly",
      "Because it requires a predefined number of clusters"
    ],
    correct: 1,
    why: "Complete linkage evaluates the worst-case distance between clusters. If any far pair exists, the merge is considered risky and delayed, whereas single linkage would merge early based on a single close pair."
  },
  {
    question: "In hierarchical clustering, why must the linkage rule remain fixed throughout the entire algorithm?",
    options: [
      "Because switching rules would change the meaning of previously computed distances",
      "Because linkage rules cannot be applied more than once",
      "Because hierarchical clustering only allows one merge",
      "Because distance matrices cannot be updated dynamically"
    ],
    correct: 0,
    why: "Each merge and distance update depends on the chosen linkage definition. Changing the rule mid-process would invalidate earlier comparisons, making the clustering inconsistent."
  },
  {
    question: "A dataset contains three dense groups connected by thin chains of points. Which outcome is most likely if single linkage is applied?",
    options: [
      "Three compact clusters remain separated",
      "The clusters merge into one long cluster through chains",
      "Clusters are merged based on centroid distance",
      "The algorithm stops before any merge"
    ],
    correct: 1,
    why: "Single linkage merges clusters if any connecting chain exists. Thin bridges between dense groups cause chaining, leading to one large cluster even when dense groups are conceptually distinct."
  },
  {
    question: "When comparing two candidate merges in complete linkage, what determines which merge happens first?",
    options: [
      "The merge with the largest maximum distance",
      "The merge with the smallest maximum distance",
      "The merge with the smallest minimum distance",
      "The merge with the smallest average distance"
    ],
    correct: 1,
    why: "Complete linkage computes a maximum distance for each cluster pair and selects the pair whose maximum distance is smallest. This minimizes the worst-case internal distance after merging."
  },
  {
    question: "Why is complete linkage considered more appropriate than single linkage for clustering user profiles based on similarity?",
    options: [
      "Because it prioritizes connectivity between users",
      "Because it allows chaining through weak similarities",
      "Because it ensures all users in a cluster are mutually similar",
      "Because it ignores outliers automatically"
    ],
    correct: 2,
    why: "User profile clusters should contain users that are all similar to each other. Complete linkage enforces this by blocking merges that would include very dissimilar users, unlike single linkage."
  },
  {
    question: "What practical problem arises if complete linkage is used on data with long continuous shapes?",
    options: [
      "The algorithm fails to compute distances",
      "The shape is split into multiple clusters despite being continuous",
      "All points merge into a single cluster",
      "Distances become asymmetric"
    ],
    correct: 1,
    why: "Complete linkage prioritizes compactness. In long or curved shapes, far endpoints cause merges to be delayed or blocked, splitting a continuous structure into separate clusters."
  },
  {
    question: "In single linkage, why can two very distant points end up in the same final cluster?",
    options: [
      "Because distance matrices ignore diagonal values",
      "Because centroid distances shrink over time",
      "Because a sequence of short local connections links them",
      "Because maximum distances are minimized"
    ],
    correct: 2,
    why: "Single linkage only requires a chain of locally close points. Even if endpoints are far apart, they become connected through intermediate points and end up in the same cluster."
  },
  {
    question: "Which statement best explains why complete linkage reduces the chaining effect?",
    options: [
      "It merges clusters based on minimum distances",
      "It avoids recalculating distances after merges",
      "It evaluates merges using worst-case separation",
      "It forces clusters to have equal sizes"
    ],
    correct: 2,
    why: "By using the maximum distance between clusters, complete linkage blocks merges that would create clusters with very distant members, directly preventing chaining."
  },
  {
    question: "If the same dataset is clustered using single and complete linkage, why do their dendrograms often differ significantly?",
    options: [
      "Because they start with different initial distance matrices",
      "Because they apply different stopping conditions",
      "Because they define inter-cluster distance differently",
      "Because one method is non-deterministic"
    ],
    correct: 2,
    why: "Both methods start from the same distance matrix and follow the same merge process. The only difference is how cluster distance is defined, which leads to different merge orders and dendrogram shapes."
  },
    {
    question: "You are clustering GPS points collected along a winding mountain road. The road curves back close to itself in places, but endpoints are far apart. Which linkage choice best preserves the road as a single structure, and why?",
    options: [
      "Complete linkage, because it minimizes the maximum distance",
      "Single linkage, because it preserves connectivity through local proximity",
      "Complete linkage, because it avoids chaining",
      "Average linkage, because it ignores extreme distances"
    ],
    correct: 1,
    why: "This is a connectivity problem. Points along a road are locally close even if endpoints are far apart. Single linkage merges clusters based on the smallest local connections, preserving continuous shapes. Complete linkage would split the road because the far endpoints would dominate the maximum distance."
  },
  {
    question: "In fraud detection, transactions form dense groups but are sometimes connected by a single unusual transaction. Why would complete linkage be preferred here?",
    options: [
      "Because it aggressively merges clusters to find fraud faster",
      "Because it blocks merges caused by weak or accidental bridges",
      "Because it follows transaction chains more accurately",
      "Because it reduces computation time"
    ],
    correct: 1,
    why: "Fraud detection prioritizes cluster purity. A single odd transaction should not merge two otherwise distinct groups. Complete linkage evaluates worst-case separation and prevents such weak bridges from causing merges, unlike single linkage."
  },
  {
    question: "A dataset contains two compact clusters connected by a thin line of sparse points. What outcome is most likely under single linkage?",
    options: [
      "The compact clusters remain separate",
      "The sparse points are ignored",
      "All points merge into one cluster via chaining",
      "The algorithm stops early due to ambiguity"
    ],
    correct: 2,
    why: "Single linkage merges clusters as long as a chain of close points exists. The sparse connecting points act as bridges, causing the two compact clusters to merge into one, even if they are conceptually different."
  },
  {
    question: "You want clusters where every member is reasonably similar to every other member. Which linkage definition directly enforces this requirement?",
    options: [
      "Single linkage using minimum distance",
      "Complete linkage using maximum distance",
      "Single linkage using global minimum",
      "Centroid linkage using mean distance"
    ],
    correct: 1,
    why: "Complete linkage uses the maximum pairwise distance as the cluster distance. This enforces a bound on how far apart any two points in a cluster can be, directly supporting mutual similarity."
  },
  {
    question: "In a recommendation system, users form long similarity chains but differ greatly at the ends. What risk does single linkage introduce?",
    options: [
      "Loss of connectivity between users",
      "Formation of clusters with very dissimilar users",
      "Excessive number of small clusters",
      "Failure to compute distances"
    ],
    correct: 1,
    why: "Single linkage can chain users together through intermediate similarities. This may place users with very different preferences into the same cluster, reducing recommendation quality."
  },
  {
    question: "Why does complete linkage often produce more balanced dendrograms than single linkage?",
    options: [
      "Because it merges clusters randomly",
      "Because it delays merges that increase internal spread",
      "Because it stops clustering earlier",
      "Because it averages distances across clusters"
    ],
    correct: 1,
    why: "Complete linkage evaluates merges by worst-case separation. This delays risky merges and results in more balanced, compact clusters, reflected in a more evenly structured dendrogram."
  },
  {
    question: "You are clustering sensor readings along a pipeline to detect continuity breaks. Which linkage choice aligns best with this goal?",
    options: [
      "Complete linkage, because it enforces compactness",
      "Single linkage, because it detects continuous connectivity",
      "Complete linkage, because it minimizes variance",
      "Average linkage, because it smooths noise"
    ],
    correct: 1,
    why: "Detecting continuity relies on whether points are connected through local proximity. Single linkage captures this by merging based on the nearest connections, making it ideal for pipelines or networks."
  },
  {
    question: "In image clustering, why might complete linkage outperform single linkage when images contain gradual background transitions?",
    options: [
      "Because it ignores local similarities",
      "Because it prevents chaining through gradual changes",
      "Because it clusters based on centroids only",
      "Because it merges clusters faster"
    ],
    correct: 1,
    why: "Gradual background transitions can form chains of locally similar images. Single linkage would chain them together, while complete linkage blocks merges if overall dissimilarity becomes large, preserving meaningful visual groups."
  },
  {
    question: "What structural property of data most strongly suggests using single linkage over complete linkage?",
    options: [
      "High dimensionality",
      "Presence of noise",
      "Existence of natural paths or curves",
      "Requirement for compact clusters"
    ],
    correct: 2,
    why: "Natural paths, curves, or networks rely on local connectivity rather than global compactness. Single linkage preserves these structures by merging through nearest neighbors."
  },
  {
    question: "Two clustering results differ significantly only because one used single linkage and the other complete linkage. What is the fundamental reason for this divergence?",
    options: [
      "Different initial distance matrices were used",
      "The stopping condition changed",
      "The definition of inter-cluster distance changed",
      "One method is approximate while the other is exact"
    ],
    correct: 2,
    why: "Both methods follow the same hierarchical process. The divergence arises solely from how inter-cluster distance is defined: minimum distance for single linkage versus maximum distance for complete linkage, leading to different merge decisions."
  },
  {
    question: "What is the main goal of a classification model?",
    options: [
      "To group data without labels",
      "To predict a continuous numeric value",
      "To assign an input to one of several predefined classes",
      "To find relationships between variables"
    ],
    correct: 2,
    why: "Classification is about decision-making between predefined categories. Grouping without labels is clustering, predicting numbers is regression, and relationship discovery is a different task altogether."
  },
  {
    question: "Which of the following is a simple real-world example of classification?",
    options: [
      "Predicting tomorrow’s temperature",
      "Grouping customers by shopping behavior without labels",
      "Deciding whether an email is spam or not spam",
      "Finding the average income of users"
    ],
    correct: 2,
    why: "Spam detection is a classic classification problem because the model chooses between two known classes. The other options involve regression or unsupervised learning."
  },
  {
    question: "What does 'supervised' mean in supervised learning?",
    options: [
      "The model supervises itself",
      "The data comes with correct labels",
      "The model learns without data",
      "The model only works on small datasets"
    ],
    correct: 1,
    why: "Supervised learning relies on labeled examples. The model learns by comparing its predictions with known correct answers."
  },
  {
    question: "Which task is NOT an example of supervised learning?",
    options: [
      "Spam detection using labeled emails",
      "House price prediction using past sales data",
      "Clustering news articles without categories",
      "Classifying images of cats and dogs"
    ],
    correct: 2,
    why: "Clustering without categories is unsupervised learning. The other tasks all use labeled data."
  },
  {
    question: "Why is probability useful in classification?",
    options: [
      "It removes the need for data",
      "It allows the model to guess randomly",
      "It helps compare how likely each class is for an input",
      "It makes models faster by default"
    ],
    correct: 2,
    why: "Probability lets the model measure uncertainty and compare how likely different classes are, instead of making hard guesses."
  },
  {
    question: "What does a classifier do when it uses probability?",
    options: [
      "It always chooses the rarest class",
      "It assigns the class with the highest probability",
      "It ignores features and uses labels only",
      "It averages all probabilities"
    ],
    correct: 1,
    why: "In probabilistic classification, the model evaluates probabilities for each class and selects the class with the highest value."
  },
  {
    question: "What is a 'prior probability' in simple terms?",
    options: [
      "The probability after seeing the data",
      "The probability of a feature given a class",
      "The probability of a class before seeing any input",
      "The probability of two features occurring together"
    ],
    correct: 2,
    why: "Prior probability represents how common a class is before considering any specific input features."
  },
  {
    question: "Why do we care about prior probabilities in classification?",
    options: [
      "They replace conditional probabilities",
      "They capture how frequent classes are in the data",
      "They remove the need for training data",
      "They guarantee correct predictions"
    ],
    correct: 1,
    why: "Priors reflect how often each class appears. Ignoring them can bias decisions, especially when classes are imbalanced."
  },
  {
    question: "What is conditional probability used for in classification?",
    options: [
      "To measure the probability of a class given observed features",
      "To count how many features exist",
      "To remove noise from data",
      "To normalize class labels"
    ],
    correct: 0,
    why: "Conditional probability expresses how likely a class is when certain features are observed, which is central to probabilistic classification."
  },
  {
    question: "Which statement best describes Naïve Bayes?",
    options: [
      "A model that ignores probability",
      "A probabilistic classifier based on Bayes’ rule",
      "A clustering algorithm",
      "A regression-only technique"
    ],
    correct: 1,
    why: "Naïve Bayes is a probabilistic classification method that applies Bayes’ rule to decide between classes."
  },
  {
    question: "Why is Naïve Bayes called 'naïve'?",
    options: [
      "Because it always makes simple predictions",
      "Because it assumes features are independent given the class",
      "Because it cannot handle real data",
      "Because it ignores training data"
    ],
    correct: 1,
    why: "The naïve part refers to the strong independence assumption between features, which simplifies probability calculations."
  },
  {
    question: "What does the independence assumption mean in Naïve Bayes?",
    options: [
      "Classes are independent of data",
      "Features are unrelated to the outcome",
      "Features are independent given the class label",
      "Training samples are independent of each other"
    ],
    correct: 2,
    why: "Naïve Bayes assumes that once the class is known, features do not depend on each other, making computations easier."
  },
  {
    question: "What decision rule does Naïve Bayes commonly use?",
    options: [
      "Minimum Error Rule",
      "Maximum Likelihood Rule",
      "Maximum A Posteriori (MAP) Rule",
      "Random Selection Rule"
    ],
    correct: 2,
    why: "Naïve Bayes typically uses the MAP rule, selecting the class with the highest posterior probability."
  },
  {
    question: "What does the MAP rule do in practice?",
    options: [
      "Chooses the class with the smallest probability",
      "Chooses the class with the highest probability",
      "Averages all class probabilities",
      "Ignores prior information"
    ],
    correct: 1,
    why: "MAP assigns the input to the class with the highest posterior probability after combining prior and conditional probabilities."
  },
  {
    question: "Which of the following best describes the training phase of Naïve Bayes?",
    options: [
      "Predicting labels for new data",
      "Computing probabilities from labeled data",
      "Removing noisy features",
      "Drawing graphs"
    ],
    correct: 1,
    why: "During training, Naïve Bayes learns probabilities such as class priors and feature likelihoods from labeled data."
  },
  {
    question: "What happens during the test phase in Naïve Bayes?",
    options: [
      "The model recomputes training labels",
      "The model compares class probabilities for a new input",
      "The model removes features",
      "The model changes its assumptions"
    ],
    correct: 1,
    why: "In testing, the model computes probabilities for each class given a new instance and selects the most likely class."
  },
  {
    question: "Why can Naïve Bayes work well even with its strong independence assumption?",
    options: [
      "Because independence always holds in real data",
      "Because exact probabilities are unnecessary for correct ranking",
      "Because it ignores irrelevant features",
      "Because it requires very large datasets"
    ],
    correct: 1,
    why: "Even when independence is violated, Naïve Bayes often ranks class probabilities correctly, which is enough for classification."
  },
  {
    question: "Which learning setting uses both labeled and unlabeled data?",
    options: [
      "Supervised learning",
      "Unsupervised learning",
      "Semi-supervised learning",
      "Reinforcement learning"
    ],
    correct: 2,
    why: "Semi-supervised learning combines a small amount of labeled data with a large amount of unlabeled data."
  },
  {
    question: "Why is labeled data important in supervised learning?",
    options: [
      "It allows the model to evaluate correctness",
      "It removes the need for algorithms",
      "It guarantees perfect accuracy",
      "It replaces probability"
    ],
    correct: 0,
    why: "Labels provide the ground truth that guides learning, allowing the model to adjust based on correct and incorrect predictions."
  },
  {
    question: "Which statement best captures the main idea of this topic so far?",
    options: [
      "Learning is mostly about choosing complex algorithms",
      "Classification uses probability to make informed decisions",
      "Graphs are required for all classifiers",
      "Unlabeled data is always better than labeled data"
    ],
    correct: 1,
    why: "The core idea is that classification, especially with Naïve Bayes, uses probability to decide which class an input most likely belongs to."
  },
  {
    question: "What problem does probabilistic classification primarily solve?",
    options: [
      "Finding exact rules with zero error",
      "Handling uncertainty when assigning classes",
      "Reducing the number of features",
      "Speeding up clustering algorithms"
    ],
    correct: 1,
    why: "Probabilistic classification is designed to handle uncertainty. Instead of making hard decisions, it evaluates how likely each class is and chooses the most probable one."
  },
  {
    question: "Why is Bayes’ rule important in classification?",
    options: [
      "It converts probabilities into distances",
      "It allows updating beliefs using observed data",
      "It removes the need for training data",
      "It guarantees optimal accuracy"
    ],
    correct: 1,
    why: "Bayes’ rule provides a systematic way to update prior beliefs about classes when new evidence (features) is observed."
  },
  {
    question: "In Naïve Bayes, what is multiplied together during prediction?",
    options: [
      "Feature values only",
      "Prior probabilities only",
      "Conditional probabilities of features given a class",
      "Distances between data points"
    ],
    correct: 2,
    why: "Naïve Bayes multiplies conditional probabilities of each feature given the class, along with the class prior, to compute the posterior score."
  },
  {
    question: "Why is multiplication of probabilities used instead of addition in Naïve Bayes?",
    options: [
      "Because probabilities must always be added",
      "Because features are assumed independent given the class",
      "Because addition would give larger values",
      "Because Bayes’ rule requires it arbitrarily"
    ],
    correct: 1,
    why: "Under the independence assumption, the joint probability of features given a class is the product of their individual conditional probabilities."
  },
  {
    question: "What would happen if two features are highly dependent in Naïve Bayes?",
    options: [
      "The algorithm stops working completely",
      "The probabilities become exactly zero",
      "The model may overcount evidence",
      "The class priors disappear"
    ],
    correct: 2,
    why: "When features are dependent, Naïve Bayes may count the same evidence multiple times, but it can still perform well in practice."
  },
  {
    question: "Which of the following best describes a discriminative classifier?",
    options: [
      "It models how data is generated for each class",
      "It directly learns decision boundaries between classes",
      "It ignores class labels",
      "It only works with probabilities"
    ],
    correct: 1,
    why: "Discriminative models focus on separating classes by learning boundaries, rather than modeling how data is generated."
  },
  {
    question: "Why is Naïve Bayes considered a generative model?",
    options: [
      "Because it generates random data",
      "Because it models the data distribution within each class",
      "Because it creates new features",
      "Because it does not require labels"
    ],
    correct: 1,
    why: "Naïve Bayes models how data is generated within each class by learning class-conditional probabilities."
  },
  {
    question: "What does MAP classification compare?",
    options: [
      "Likelihoods only",
      "Priors only",
      "Posterior probabilities of classes",
      "Distances between centroids"
    ],
    correct: 2,
    why: "MAP (Maximum A Posteriori) classification compares posterior probabilities and selects the class with the highest value."
  },
  {
    question: "If two classes have equal conditional probabilities for an input, what role do priors play?",
    options: [
      "They become irrelevant",
      "They break the tie by favoring the more common class",
      "They cancel out automatically",
      "They are ignored by MAP"
    ],
    correct: 1,
    why: "When likelihoods are equal, the prior probabilities determine which class is more likely overall."
  },
  {
    question: "Why can Naïve Bayes be trained quickly compared to many other models?",
    options: [
      "Because it does not use probabilities",
      "Because it only counts occurrences to estimate probabilities",
      "Because it avoids feature extraction",
      "Because it ignores class labels"
    ],
    correct: 1,
    why: "Training Naïve Bayes mainly involves counting how often features occur in each class, making it computationally efficient."
  },
  {
    question: "What is a common practical use of Naïve Bayes?",
    options: [
      "Image segmentation",
      "Spam filtering",
      "Clustering social networks",
      "Sorting numbers"
    ],
    correct: 1,
    why: "Naïve Bayes is widely used in spam filtering due to its simplicity and strong performance on text data."
  },
  {
    question: "Why is Naïve Bayes suitable for text classification?",
    options: [
      "Text features are always independent",
      "Text data naturally fits probabilistic models",
      "It handles high-dimensional feature spaces well",
      "It requires very small vocabularies"
    ],
    correct: 2,
    why: "Text data often has many features (words). Naïve Bayes handles high-dimensional spaces efficiently despite the independence assumption."
  },
  {
    question: "What happens if a conditional probability is zero in Naïve Bayes?",
    options: [
      "The class probability becomes zero",
      "The model crashes",
      "The feature is ignored",
      "The prior is increased"
    ],
    correct: 0,
    why: "Because probabilities are multiplied, a single zero conditional probability will make the entire class probability zero."
  },
  {
    question: "Why is smoothing often used in Naïve Bayes?",
    options: [
      "To increase computation speed",
      "To avoid zero probabilities for unseen feature values",
      "To remove irrelevant classes",
      "To enforce independence"
    ],
    correct: 1,
    why: "Smoothing prevents zero probabilities when a feature value was not observed in the training data for a class."
  },
  {
    question: "Which step comes first in a supervised learning workflow?",
    options: [
      "Choosing an algorithm",
      "Collecting labeled data",
      "Modeling the problem",
      "Testing the model"
    ],
    correct: 2,
    why: "Before selecting data or algorithms, the problem must be clearly defined: what is being predicted and how success is measured."
  },
  {
    question: "Why is feature extraction considered critical in supervised learning?",
    options: [
      "Because algorithms cannot work without it",
      "Because good features make class separation easier",
      "Because it replaces training data",
      "Because it guarantees high accuracy"
    ],
    correct: 1,
    why: "Well-chosen features capture relevant information and make it easier for classifiers to distinguish between classes."
  },
  {
    question: "What risk comes with adding too many features?",
    options: [
      "Lower memory usage",
      "Need for less training data",
      "Overfitting and data sparsity",
      "Automatic feature selection"
    ],
    correct: 2,
    why: "More features increase model complexity and usually require more training data to avoid overfitting."
  },
  {
    question: "Why is obtaining labeled data often difficult in practice?",
    options: [
      "Labels can only be generated automatically",
      "It requires manual effort and domain expertise",
      "Algorithms do not accept labeled data",
      "Labels reduce model accuracy"
    ],
    correct: 1,
    why: "Labeling data typically requires human effort, expertise, and careful planning, which makes it expensive and slow."
  },
  {
    question: "What is the main idea behind semi-supervised learning?",
    options: [
      "Using only unlabeled data",
      "Combining labeled and unlabeled data",
      "Replacing probability with rules",
      "Avoiding feature extraction"
    ],
    correct: 1,
    why: "Semi-supervised learning leverages a small amount of labeled data together with a large amount of unlabeled data to improve learning."
  },
  {
    question: "What is the key takeaway from this chunk?",
    options: [
      "Naïve Bayes is mathematically complex",
      "Probability helps compare and choose between classes",
      "Graphs are central to all classifiers",
      "Training data quality does not matter"
    ],
    correct: 1,
    why: "This chunk reinforces how probability, Bayes’ rule, and supervised learning principles work together to make informed classification decisions."
  },
  {
    question: "What distinguishes supervised learning from unsupervised learning at the most basic level?",
    options: [
      "Supervised learning uses graphs, unsupervised does not",
      "Supervised learning requires labeled data",
      "Supervised learning is always probabilistic",
      "Supervised learning cannot scale to large data"
    ],
    correct: 1,
    why: "The defining difference is labels. Supervised learning learns from examples with known correct outputs, while unsupervised learning discovers structure without labels."
  },
  {
    question: "Which task best fits supervised learning rather than unsupervised learning?",
    options: [
      "Finding communities in a social network",
      "Grouping documents without categories",
      "Predicting whether a transaction is fraudulent",
      "Discovering hidden patterns in unlabeled data"
    ],
    correct: 2,
    why: "Fraud prediction requires known labels (fraud or not fraud) during training, which makes it a supervised learning task."
  },
  {
    question: "Why is modeling the problem considered the first step in supervised learning?",
    options: [
      "Because algorithms cannot work without it",
      "Because it defines what you are trying to predict",
      "Because it reduces the dataset size",
      "Because it selects the linkage method"
    ],
    correct: 1,
    why: "Before choosing data or algorithms, you must clearly define the prediction goal and what output the model should produce."
  },
  {
    question: "What is the main purpose of feature extraction?",
    options: [
      "To reduce computation only",
      "To remove the need for labels",
      "To represent data in a way that helps separate classes",
      "To replace the learning algorithm"
    ],
    correct: 2,
    why: "Features capture the information that helps distinguish between classes. Good features often matter more than the choice of algorithm."
  },
  {
    question: "Why does adding more features often require more training data?",
    options: [
      "Because probabilities stop working",
      "Because models become simpler",
      "Because data sparsity and overfitting increase",
      "Because labels become unnecessary"
    ],
    correct: 2,
    why: "More features increase model complexity, which requires more data to learn reliable patterns and avoid overfitting."
  },
  {
    question: "What practical challenge commonly arises when collecting labeled data?",
    options: [
      "Labels are always inaccurate",
      "Labels require manual effort and expertise",
      "Labels cannot be used by classifiers",
      "Labels remove uncertainty"
    ],
    correct: 1,
    why: "Labeling often needs human judgment and domain knowledge, making it time-consuming and expensive."
  },
  {
    question: "Why are semi-supervised learning techniques useful?",
    options: [
      "They eliminate the need for algorithms",
      "They work without any labeled data",
      "They leverage a small labeled set with large unlabeled data",
      "They guarantee higher accuracy"
    ],
    correct: 2,
    why: "Semi-supervised learning is useful when labeled data is scarce but unlabeled data is abundant, which is common in real-world problems."
  },
  {
    question: "What is the main idea behind self-training in semi-supervised learning?",
    options: [
      "Training multiple independent models",
      "Using high-confidence predictions as new labels",
      "Ignoring unlabeled data",
      "Replacing probabilities with rules"
    ],
    correct: 1,
    why: "Self-training uses the model’s confident predictions on unlabeled data as additional training examples."
  },
  {
    question: "Why is testing a model in practice different from training it?",
    options: [
      "Testing changes the training labels",
      "Testing evaluates performance on unseen data",
      "Testing removes features",
      "Testing makes the model probabilistic"
    ],
    correct: 1,
    why: "Testing checks how well the learned model generalizes to new, unseen data rather than memorizing training examples."
  },
  {
    question: "What problem does A/B testing help solve?",
    options: [
      "Training models faster",
      "Choosing between two model variants in live systems",
      "Labeling data automatically",
      "Reducing feature dimensionality"
    ],
    correct: 1,
    why: "A/B testing compares two versions of a model or system in real use to see which performs better."
  },
  {
    question: "Why does having more data often outperform using more complex algorithms?",
    options: [
      "Because algorithms stop working on large data",
      "Because data reduces noise and variance",
      "Because probability becomes irrelevant",
      "Because features are ignored"
    ],
    correct: 1,
    why: "More data provides better coverage of the problem space, often improving performance more than algorithmic complexity."
  },
  {
    question: "What is a graph primarily used to represent in machine learning contexts?",
    options: [
      "Only numerical values",
      "Independent data points",
      "Relationships between entities",
      "Feature probabilities"
    ],
    correct: 2,
    why: "Graphs are designed to represent entities and the relationships or connections between them."
  },
  {
    question: "In an undirected graph, what does the degree of a node represent?",
    options: [
      "The direction of edges",
      "The number of edges connected to the node",
      "The importance score of the node",
      "The probability of the node"
    ],
    correct: 1,
    why: "Degree counts how many edges are incident to a node, indicating how connected it is."
  },
  {
    question: "What is a connected component in an undirected graph?",
    options: [
      "A set of nodes with no edges",
      "A set of nodes where each is reachable from any other",
      "A single isolated node only",
      "A directed cycle"
    ],
    correct: 1,
    why: "A connected component consists of nodes that can all reach each other through some path."
  },
  {
    question: "How does a directed graph differ from an undirected graph?",
    options: [
      "Directed graphs have no paths",
      "Edges in directed graphs have direction",
      "Directed graphs cannot be connected",
      "Directed graphs ignore nodes"
    ],
    correct: 1,
    why: "In directed graphs, edges go from one node to another in a specific direction, affecting reachability."
  },
  {
    question: "What does a strongly connected component represent in a directed graph?",
    options: [
      "Nodes connected by undirected paths",
      "Nodes with no incoming edges",
      "Nodes where each can reach every other via directed paths",
      "Nodes with the highest degree"
    ],
    correct: 2,
    why: "Strong connectivity requires mutual reachability using directed paths among all nodes in the component."
  },
  {
    question: "What defines a bipartite graph?",
    options: [
      "Edges have directions",
      "Nodes are divided into two sets with no internal edges",
      "All nodes are connected to each other",
      "Edges form cycles only"
    ],
    correct: 1,
    why: "In a bipartite graph, nodes are split into two sets and edges only connect nodes from different sets."
  },
  {
    question: "Why is node importance a key problem in graph analysis?",
    options: [
      "Because all nodes are equally important",
      "Because it helps identify influential or authoritative entities",
      "Because it removes edges from graphs",
      "Because it simplifies probability models"
    ],
    correct: 1,
    why: "Identifying important nodes helps find influential users, authoritative pages, or central entities in a network."
  },
  {
    question: "Which real-world problem best matches graph-based importance analysis?",
    options: [
      "Sorting numbers",
      "Ranking web pages",
      "Predicting house prices",
      "Clustering unlabeled images"
    ],
    correct: 1,
    why: "Web page ranking depends on link structure, making it a graph-based importance problem."
  },
  {
    question: "What is the main takeaway from this final chunk?",
    options: [
      "Algorithms matter more than data",
      "Learning problems vary by data type and structure",
      "Graphs replace classification models",
      "Probability is only used in Naïve Bayes"
    ],
    correct: 1,
    why: "This chunk shows that different learning problems require different representations—features for classification and structure for graphs."
  },
  {
    question: "What does Minimum Edit Distance measure between two strings?",
    options: [
      "The number of matching characters in the same positions",
      "The minimum number of edits needed to transform one string into another",
      "The length difference between two strings",
      "The probability that two strings are related"
    ],
    correct: 1,
    why: "Minimum Edit Distance counts the smallest number of editing operations required to convert one string into another. It is not about matching positions or probabilities, but about transformation cost."
  },
  {
    question: "Which set of operations is used in defining Minimum Edit Distance?",
    options: [
      "Copy, move, replace",
      "Insert, delete, substitute",
      "Merge, split, reorder",
      "Match, skip, align"
    ],
    correct: 1,
    why: "Edit distance is defined using three basic operations: insertion, deletion, and substitution. These are sufficient to transform any string into another."
  },
  {
    question: "Why is Minimum Edit Distance useful in spell correction?",
    options: [
      "It finds words with the same length",
      "It ranks dictionary words by how easily they can be transformed from the misspelled word",
      "It checks grammar rules",
      "It counts how often a word appears"
    ],
    correct: 1,
    why: "In spell correction, the correct word is often the one requiring the fewest edits from the typed word. Edit distance provides a principled way to rank candidate corrections."
  },
  {
    question: "If all edit operations have cost 1, what does the edit distance represent?",
    options: [
      "The total number of characters in both strings",
      "The minimum number of edits regardless of type",
      "Only the number of substitutions",
      "Only the number of insertions"
    ],
    correct: 1,
    why: "When all operations cost 1, edit distance simply counts how many edits—of any type—are needed to transform one string into the other."
  },
  {
    question: "Why might substitutions be assigned a higher cost than insertions or deletions?",
    options: [
      "To make the algorithm faster",
      "To discourage changing characters when insert/delete is more appropriate",
      "To reduce memory usage",
      "To force alignments to be longer"
    ],
    correct: 1,
    why: "Assigning a higher cost to substitution reflects the idea that replacing a character may be more significant than inserting or deleting one, influencing which transformations are preferred."
  },
  {
    question: "In computational biology, what is a common use of edit distance?",
    options: [
      "Counting gene frequencies",
      "Aligning DNA or protein sequences",
      "Predicting protein structures directly",
      "Classifying organisms"
    ],
    correct: 1,
    why: "Edit distance is used to align biological sequences by measuring how many insertions, deletions, or substitutions are needed to make them match."
  },
  {
    question: "What does an alignment represent in the context of edit distance?",
    options: [
      "A list of probabilities for each character",
      "A one-to-one matching of characters and gaps between two strings",
      "Only the final distance value",
      "A sorted version of both strings"
    ],
    correct: 1,
    why: "An alignment shows how characters (or gaps) from one string correspond to characters (or gaps) in the other, explaining how the edit distance was achieved."
  },
  {
    question: "Why is edit distance applicable to machine translation and speech recognition evaluation?",
    options: [
      "Because it measures semantic meaning",
      "Because it measures surface-level differences between outputs",
      "Because it replaces human evaluation completely",
      "Because it requires no reference output"
    ],
    correct: 1,
    why: "Edit distance measures how different a system’s output is from a reference at the character or word level, making it useful for evaluation tasks."
  },
  {
    question: "Which statement best captures the core idea behind Minimum Edit Distance?",
    options: [
      "Similarity is based on shared features",
      "Similarity is based on minimal transformation effort",
      "Similarity depends only on string length",
      "Similarity is defined by probability"
    ],
    correct: 1,
    why: "Minimum Edit Distance defines similarity in terms of how much effort it takes to transform one string into another using basic edit operations."
  },
  {
    question: "Why is edit distance considered a general-purpose similarity measure for strings?",
    options: [
      "It relies on language-specific rules",
      "It works only for short strings",
      "It applies the same transformation logic across many domains",
      "It ignores character order"
    ],
    correct: 2,
    why: "Edit distance uses simple, universal operations that apply across domains like text, biology, and speech, making it broadly useful."
  },
  {
    question: "Why can Minimum Edit Distance be viewed as a search problem?",
    options: [
      "Because it searches for matching characters",
      "Because it searches for the shortest sequence of edits from one string to another",
      "Because it searches the dictionary for similar words",
      "Because it searches for probabilities of edits"
    ],
    correct: 1,
    why: "Minimum Edit Distance can be framed as searching for a path from the start string to the goal string, where each step is an edit operation and the goal is to minimize total cost."
  },
  {
    question: "In the search formulation of edit distance, what represents the initial state?",
    options: [
      "The empty string",
      "The final target string",
      "The word we are transforming",
      "The shortest intermediate string"
    ],
    correct: 2,
    why: "The initial state is the original string we want to transform. From this state, edits are applied to reach the target string."
  },
  {
    question: "What is the goal state in Minimum Edit Distance?",
    options: [
      "Any string with the same length as the target",
      "The string with minimum cost so far",
      "The target string we want to reach",
      "A string with no substitutions"
    ],
    correct: 2,
    why: "The goal state is the exact target string. The task is to reach it using the minimum-cost sequence of edits."
  },
  {
    question: "Why is naive search over all edit sequences impractical?",
    options: [
      "Because edit operations are undefined",
      "Because the number of possible edit sequences is very large",
      "Because edit distance only works for short strings",
      "Because costs cannot be compared"
    ],
    correct: 1,
    why: "There are many different sequences of edits that can reach the same intermediate string. Exploring all of them would be computationally infeasible."
  },
  {
    question: "What key idea allows us to avoid exploring all edit paths?",
    options: [
      "Random sampling",
      "Greedy selection of edits",
      "Dynamic programming with shared subproblems",
      "Ignoring substitutions"
    ],
    correct: 2,
    why: "Dynamic programming exploits the fact that many edit paths reach the same subproblem. We only need to keep the shortest path to each subproblem."
  },
  {
    question: "What does D(i, j) represent in the edit distance algorithm?",
    options: [
      "The distance between full strings X and Y",
      "The number of edits used so far",
      "The edit distance between the first i characters of X and first j characters of Y",
      "The cost of substituting character i with j"
    ],
    correct: 2,
    why: "D(i, j) is defined as the minimum edit distance between prefixes X[1..i] and Y[1..j]. This allows the problem to be broken into smaller subproblems."
  },
  {
    question: "Why are the base cases D(i,0) = i and D(0,j) = j necessary?",
    options: [
      "They improve runtime",
      "They represent transforming a string to or from an empty string",
      "They handle substitution costs",
      "They prevent negative distances"
    ],
    correct: 1,
    why: "Transforming a string of length i into an empty string requires i deletions, and transforming an empty string into length j requires j insertions."
  },
  {
    question: "What does filling the edit distance table from top-left to bottom-right achieve?",
    options: [
      "It sorts the strings",
      "It ensures larger problems use solutions of smaller subproblems",
      "It minimizes memory usage",
      "It avoids substitutions"
    ],
    correct: 1,
    why: "Dynamic programming computes small subproblems first so that larger D(i,j) values can be computed using already known results."
  },
  {
    question: "Which operation corresponds to moving diagonally in the edit distance table?",
    options: [
      "Insertion",
      "Deletion",
      "Substitution or match",
      "Restart"
    ],
    correct: 2,
    why: "A diagonal move represents aligning two characters. If they are equal, the cost is 0; if not, it is a substitution cost."
  },
  {
    question: "Why does dynamic programming guarantee the minimum edit distance?",
    options: [
      "Because it tries all possible paths",
      "Because it uses randomization",
      "Because optimal solutions are composed of optimal subsolutions",
      "Because costs are always positive"
    ],
    correct: 2,
    why: "The edit distance problem has optimal substructure: the optimal alignment of two strings depends on optimal alignments of their prefixes, which dynamic programming captures."
  },
  {
    question: "Why is edit distance alone sometimes not sufficient in real applications?",
    options: [
      "Because it cannot handle long strings",
      "Because it does not show how characters align",
      "Because it ignores substitution costs",
      "Because it only works for text"
    ],
    correct: 1,
    why: "Edit distance gives only a numeric cost. Many applications need to know which characters were matched, inserted, or deleted, which requires an explicit alignment."
  },
  {
    question: "What is the purpose of backtrace in the edit distance algorithm?",
    options: [
      "To recompute the distance faster",
      "To recover the sequence of edits that produced the minimum distance",
      "To reduce memory usage",
      "To change the cost of operations"
    ],
    correct: 1,
    why: "Backtrace allows us to reconstruct the actual alignment by following the path of decisions that led to the minimum edit distance."
  },
  {
    question: "From which cell does backtrace usually start in the edit distance table?",
    options: [
      "The top-left corner",
      "The bottom-left corner",
      "The top-right corner",
      "The bottom-right corner"
    ],
    correct: 3,
    why: "Backtrace starts from D(n, m), which represents the full strings, and traces backward to D(0,0) to recover the alignment."
  },
  {
    question: "What does a horizontal move during backtrace represent?",
    options: [
      "Substitution",
      "Insertion",
      "Deletion",
      "Match"
    ],
    correct: 1,
    why: "A horizontal move corresponds to inserting a character into the source string to match the target string."
  },
  {
    question: "What does a vertical move during backtrace represent?",
    options: [
      "Insertion",
      "Deletion",
      "Substitution",
      "Match"
    ],
    correct: 1,
    why: "A vertical move corresponds to deleting a character from the source string."
  },
  {
    question: "What does a diagonal move during backtrace indicate?",
    options: [
      "Only insertion",
      "Only deletion",
      "Either a match or a substitution",
      "Restart of alignment"
    ],
    correct: 2,
    why: "A diagonal move aligns two characters. If they are equal, it is a match; if they differ, it represents a substitution."
  },
  {
    question: "Why is dynamic programming suitable for computing alignments?",
    options: [
      "Because alignments require sorting",
      "Because alignments can be built from optimal sub-alignments",
      "Because alignments avoid recursion",
      "Because alignments reduce string length"
    ],
    correct: 1,
    why: "An optimal alignment of two strings is composed of optimal alignments of their prefixes, which is exactly what dynamic programming exploits."
  },
  {
    question: "What is the time complexity of computing Minimum Edit Distance using dynamic programming?",
    options: [
      "O(n + m)",
      "O(nm)",
      "O(n² + m²)",
      "O(2ⁿ)"
    ],
    correct: 1,
    why: "The algorithm fills an n by m table, computing each cell once, leading to O(nm) time complexity."
  },
  {
    question: "What is the space complexity of the standard edit distance algorithm?",
    options: [
      "O(n + m)",
      "O(min(n, m))",
      "O(nm)",
      "O(1)"
    ],
    correct: 2,
    why: "The full dynamic programming table stores n × m values, resulting in O(nm) space complexity."
  },
  {
    question: "Which statement best summarizes the role of Minimum Edit Distance with backtrace?",
    options: [
      "It only measures similarity numerically",
      "It finds both the similarity score and the exact alignment",
      "It replaces probabilistic models",
      "It is only useful for spell checking"
    ],
    correct: 1,
    why: "With backtrace, Minimum Edit Distance not only computes how similar two strings are, but also explains that similarity by showing the exact sequence of edits."
  },
  {
    question: "A medical test correctly identifies a sick patient as sick. How is this outcome classified?",
    options: [
      "True Negative (TN)",
      "False Positive (FP)",
      "True Positive (TP)",
      "False Negative (FN)"
    ],
    correct: 2,
    why: "The prediction is Positive and the actual condition is also Positive. When both match and are correct, the outcome is a True Positive."
  },
  {
    question: "A spam filter marks an important email as spam. What type of error is this?",
    options: [
      "True Positive",
      "False Negative",
      "True Negative",
      "False Positive"
    ],
    correct: 3,
    why: "The model predicted Positive (spam), but the actual class was Negative (not spam). This is a False Positive, also known as a false alarm."
  },
  {
    question: "In a confusion matrix, which value represents missed detections?",
    options: [
      "True Positive (TP)",
      "False Positive (FP)",
      "True Negative (TN)",
      "False Negative (FN)"
    ],
    correct: 3,
    why: "False Negatives occur when the actual class is Positive but the model predicts Negative, meaning the model missed a real positive case."
  },
  {
    question: "If a disease screening test aims to avoid missing sick patients, which error is most dangerous?",
    options: [
      "False Positive",
      "True Positive",
      "False Negative",
      "True Negative"
    ],
    correct: 2,
    why: "False Negatives are dangerous in medical tests because a sick patient is told they are healthy, delaying treatment."
  },
  {
    question: "Given TP = 50 and FP = 10, what does Precision measure?",
    options: [
      "How many actual positives were found",
      "How many predictions were correct overall",
      "How many predicted positives were correct",
      "How many negatives were correctly rejected"
    ],
    correct: 2,
    why: "Precision focuses on prediction quality: out of all predicted positives (TP + FP), how many were actually correct."
  },
  {
    question: "Which formula correctly defines Precision?",
    options: [
      "TP / (TP + FN)",
      "TP / (TP + FP)",
      "(TP + TN) / Total",
      "2TP / (2TP + FP + FN)"
    ],
    correct: 1,
    why: "Precision is defined as TP divided by all predicted positives (TP + FP). It answers: when the model predicts positive, how often is it right?"
  },
  {
    question: "A model catches almost all spam emails but also flags many normal emails as spam. Which metric is likely high?",
    options: [
      "Precision",
      "Accuracy",
      "Recall",
      "Specificity"
    ],
    correct: 2,
    why: "Catching almost all spam means very few False Negatives, which results in high Recall. However, many false alarms reduce Precision."
  },
  {
    question: "Given TP = 30 and FN = 10, what does Recall measure?",
    options: [
      "How many predicted positives were correct",
      "How many actual positives were detected",
      "How many negatives were correctly classified",
      "Overall correctness of the model"
    ],
    correct: 1,
    why: "Recall measures coverage of positives: out of all actual positives (TP + FN), how many were correctly detected."
  },
  {
    question: "Which formula correctly defines Recall?",
    options: [
      "TP / (TP + FP)",
      "TP / (TP + FN)",
      "(TP + TN) / Total",
      "FP / (FP + TN)"
    ],
    correct: 1,
    why: "Recall is TP divided by all actual positives (TP + FN). It shows how well the model avoids missing positives."
  },
  {
    question: "In a cancer test, the doctor says: 'If you are sick, I want to be sure the test detects it.' Which metric matters most?",
    options: [
      "Precision",
      "Accuracy",
      "Recall",
      "F1-score"
    ],
    correct: 2,
    why: "In disease detection, missing a sick patient is dangerous. Recall is critical because it minimizes False Negatives."
  },
  {
    question: "A classifier has TP=20, FP=2, FN=18. Which statement best describes its behavior?",
    options: [
      "High recall and high precision",
      "High precision but low recall",
      "Low precision but high recall",
      "Low precision and low recall"
    ],
    correct: 1,
    why: "Precision = TP/(TP+FP) = 20/22 ≈ high, while Recall = TP/(TP+FN) = 20/38 ≈ low. Few false alarms, many misses."
  },
  {
    question: "Why is accuracy a poor metric when classes are highly imbalanced?",
    options: [
      "Accuracy ignores true negatives",
      "Accuracy can be high even if positives are mostly missed",
      "Accuracy cannot be computed from a confusion matrix",
      "Accuracy favors probabilistic models only"
    ],
    correct: 1,
    why: "With imbalanced data, predicting the majority class can yield high accuracy while failing to detect the minority class, which is often the real objective."
  },
  {
    question: "Which metric best balances Precision and Recall into a single score?",
    options: [
      "Accuracy",
      "Specificity",
      "F1-score",
      "Error rate"
    ],
    correct: 2,
    why: "F1-score is the harmonic mean of Precision and Recall, penalizing extreme imbalance between them."
  },
  {
    question: "Why does F1-score use the harmonic mean instead of the arithmetic mean?",
    options: [
      "It simplifies computation",
      "It gives more weight to larger values",
      "It penalizes cases where one metric is very low",
      "It ignores false positives"
    ],
    correct: 2,
    why: "The harmonic mean drops sharply when either Precision or Recall is low, enforcing a balanced performance."
  },
  {
    question: "Given Precision = 0.9 and Recall = 0.1, what does a low F1-score indicate?",
    options: [
      "The model is excellent overall",
      "The model predicts negatives well",
      "The model is unbalanced and misses many positives",
      "The model has high accuracy"
    ],
    correct: 2,
    why: "Despite high precision, very low recall means many positives are missed; F1 highlights this imbalance."
  },
  {
    question: "In legal verdict prediction, which metric is usually prioritized and why?",
    options: [
      "Recall, to catch all guilty cases",
      "Precision, to avoid false accusations",
      "Accuracy, to maximize correctness",
      "F1-score, to balance errors"
    ],
    correct: 1,
    why: "False positives (wrongly accusing someone) are very costly, so precision is prioritized."
  },
  {
    question: "In disease screening, which trade-off is usually acceptable?",
    options: [
      "More false negatives to reduce cost",
      "More false positives to avoid missing cases",
      "Lower recall to increase accuracy",
      "Lower precision to increase specificity"
    ],
    correct: 1,
    why: "Screening prefers high recall to avoid missing sick patients, even if it causes more false alarms."
  },
  {
    question: "If a model improves Recall but Precision drops, what likely changed?",
    options: [
      "The decision threshold became stricter",
      "The model predicts fewer positives",
      "The decision threshold became looser",
      "True negatives increased"
    ],
    correct: 2,
    why: "Lowering the threshold predicts more positives, catching more true positives (higher recall) but also increasing false positives (lower precision)."
  },
  {
    question: "Which confusion matrix values directly affect Precision?",
    options: [
      "TP and FN",
      "TN and FN",
      "TP and FP",
      "TN and FP"
    ],
    correct: 2,
    why: "Precision is TP/(TP+FP), so only True Positives and False Positives matter."
  },
  {
    question: "Which confusion matrix values directly affect Recall?",
    options: [
      "TP and FP",
      "TP and FN",
      "TN and FP",
      "TN and FN"
    ],
    correct: 1,
    why: "Recall is TP/(TP+FN), focusing on how many actual positives were correctly detected."
  },
  {
    question: "What is the main purpose of defining similarity or distance between objects?",
    options: [
      "To classify data automatically",
      "To measure how close or alike two objects are",
      "To reduce the size of datasets",
      "To replace machine learning models"
    ],
    correct: 1,
    why: "Similarity measures quantify how close two objects are. Many tasks like recommendation, clustering, and duplicate detection depend on this notion of closeness."
  },
  {
    question: "Which task best illustrates the need for a similarity measure?",
    options: [
      "Sorting numbers in ascending order",
      "Finding similar products bought by customers",
      "Training a neural network",
      "Encrypting user data"
    ],
    correct: 1,
    why: "Recommending similar products requires measuring how alike items are, which is exactly what similarity measures provide."
  },
  {
    question: "Why does the definition of similarity depend on the type of data?",
    options: [
      "Because similarity is subjective",
      "Because different data types require different notions of closeness",
      "Because similarity only works for numbers",
      "Because distance is always Euclidean"
    ],
    correct: 1,
    why: "Text, sets, and vectors have different structures, so similarity must be defined in a way that matches the data representation."
  },
  {
    question: "If two documents share many common words, which basic similarity idea applies?",
    options: [
      "Vector alignment",
      "Edit distance",
      "Intersection-based similarity",
      "Orthogonality"
    ],
    correct: 2,
    why: "Intersection-based similarity counts how many elements (words) two sets share, making it suitable for basic document comparison."
  },
  {
    question: "Which document is most similar to 'apple releases new ipod' using word intersection?",
    options: [
      "new apple pie recipe",
      "apple releases new ipad",
      "microsoft releases new product",
      "recipe for apple pie"
    ],
    correct: 1,
    why: "This document shares most words ('apple', 'releases', 'new') with the original, giving the largest intersection."
  },
  {
    question: "Why is raw intersection count sometimes misleading?",
    options: [
      "Because it ignores word order",
      "Because longer documents naturally have larger intersections",
      "Because it cannot be computed efficiently",
      "Because it requires probabilities"
    ],
    correct: 1,
    why: "Longer documents tend to share more words by chance, so intersection alone does not normalize for document size."
  },
  {
    question: "What problem does Jaccard similarity solve compared to raw intersection?",
    options: [
      "It increases similarity scores",
      "It normalizes similarity by total unique elements",
      "It ignores common elements",
      "It converts sets into vectors"
    ],
    correct: 1,
    why: "Jaccard similarity divides intersection by union, normalizing similarity relative to document size."
  },
  {
    question: "What does a Jaccard similarity of 1 mean?",
    options: [
      "The sets are unrelated",
      "The sets share some elements",
      "The sets are identical",
      "The sets are orthogonal"
    ],
    correct: 2,
    why: "Jaccard similarity equals 1 only when the two sets are exactly the same."
  },
  {
    question: "What does a Jaccard similarity of 0 indicate?",
    options: [
      "Partial overlap",
      "Complete mismatch",
      "Same size sets",
      "High similarity"
    ],
    correct: 1,
    why: "A Jaccard similarity of 0 means the two sets have no elements in common."
  },
  {
    question: "Why is Jaccard similarity symmetric?",
    options: [
      "Because union is larger than intersection",
      "Because intersection and union are independent of order",
      "Because vectors are normalized",
      "Because it ignores duplicates"
    ],
    correct: 1,
    why: "The intersection and union of two sets are the same regardless of order, making Jaccard similarity symmetric."
  },
  {
    question: "Why might representing documents as vectors be more informative than sets?",
    options: [
      "Vectors preserve word order",
      "Vectors can capture frequency and direction",
      "Vectors eliminate noise automatically",
      "Vectors avoid normalization"
    ],
    correct: 1,
    why: "Vector representations can encode word frequency and allow geometric comparison, which set-based methods cannot."
  },
  {
    question: "What does cosine similarity primarily measure between two vectors?",
    options: [
      "Difference in magnitude",
      "Difference in length",
      "Angle between the vectors",
      "Distance between centroids"
    ],
    correct: 2,
    why: "Cosine similarity measures how aligned two vectors are by computing the cosine of the angle between them."
  },
  {
    question: "What does cosine similarity equal when two vectors point in the same direction?",
    options: [
      "0",
      "0.5",
      "1",
      "-1"
    ],
    correct: 2,
    why: "When vectors are perfectly aligned, the angle between them is 0°, and cosine(0°) = 1."
  },
  {
    question: "What does cosine similarity equal when two vectors are orthogonal?",
    options: [
      "1",
      "0",
      "-1",
      "Undefined"
    ],
    correct: 1,
    why: "Orthogonal vectors have a 90° angle between them, and cosine(90°) = 0, meaning no similarity."
  },
  {
    question: "Why is cosine similarity commonly used for document comparison?",
    options: [
      "It ignores document length",
      "It emphasizes rare words",
      "It works only for text",
      "It requires binary vectors"
    ],
    correct: 0,
    why: "Cosine similarity normalizes by vector length, so long documents are not unfairly favored."
  },
  {
    question: "If two documents share topics but differ in length, which measure works best?",
    options: [
      "Edit distance",
      "Intersection count",
      "Cosine similarity",
      "Hamming distance"
    ],
    correct: 2,
    why: "Cosine similarity captures semantic alignment while being insensitive to document length."
  },
  {
    question: "What does the dot product represent in cosine similarity?",
    options: [
      "Total word count",
      "Number of shared words",
      "Degree of alignment between vectors",
      "Distance between vectors"
    ],
    correct: 2,
    why: "The dot product captures how much two vectors align along the same dimensions."
  },
  {
    question: "Why are vectors normalized in cosine similarity?",
    options: [
      "To reduce computation",
      "To remove negative values",
      "To ensure similarity depends on direction, not magnitude",
      "To convert vectors into sets"
    ],
    correct: 2,
    why: "Normalization ensures similarity reflects orientation (semantic similarity), not raw size."
  },
  {
    question: "If cosine similarity between two documents is close to 0, what does it imply?",
    options: [
      "They are identical",
      "They share many words",
      "They are semantically unrelated",
      "They differ only in length"
    ],
    correct: 2,
    why: "A cosine value near 0 indicates little to no alignment between the vectors."
  },
  {
    question: "Which similarity measure captures semantic similarity better than literal similarity?",
    options: [
      "Edit distance",
      "Jaccard similarity",
      "Cosine similarity",
      "Intersection count"
    ],
    correct: 2,
    why: "Cosine similarity captures meaning alignment rather than exact string overlap."
  },
  {
    question: "Which similarity measure is best for spelling correction?",
    options: [
      "Cosine similarity",
      "Jaccard similarity",
      "Edit distance",
      "Vector dot product"
    ],
    correct: 2,
    why: "Spelling correction depends on literal character changes, which edit distance directly measures."
  },
  {
    question: "Which similarity measure is best for detecting near-duplicate web pages?",
    options: [
      "Edit distance",
      "Cosine similarity",
      "Jaccard similarity",
      "Euclidean distance"
    ],
    correct: 2,
    why: "Near-duplicate detection focuses on overlapping content, which Jaccard similarity captures well."
  },
  {
    question: "Why is cosine similarity preferred over edit distance for sentence meaning?",
    options: [
      "Edit distance ignores words",
      "Cosine captures semantic alignment, not character edits",
      "Cosine works only for long strings",
      "Edit distance is slower"
    ],
    correct: 1,
    why: "Cosine similarity captures meaning via vector alignment, while edit distance focuses on surface form."
  },
  {
    question: "If two sentences mean the same thing but use different words, which measure works best?",
    options: [
      "Edit distance",
      "Jaccard similarity",
      "Cosine similarity",
      "Intersection size"
    ],
    correct: 2,
    why: "Cosine similarity can capture semantic similarity even with different wording."
  },
  {
    question: "Which similarity approach treats documents as directions in space?",
    options: [
      "Edit distance",
      "Set similarity",
      "Vector similarity",
      "String matching"
    ],
    correct: 2,
    why: "Vector-based similarity treats documents as points or directions in high-dimensional space."
  },
  {
    question: "What is the main weakness of Jaccard similarity?",
    options: [
      "It is asymmetric",
      "It ignores frequency information",
      "It cannot be computed efficiently",
      "It depends on order"
    ],
    correct: 1,
    why: "Jaccard treats elements as binary (present/absent) and ignores how often they occur."
  },
  {
    question: "Which measure would incorrectly rate these as similar: 'apple apple apple' and 'apple'?",
    options: [
      "Cosine similarity",
      "Edit distance",
      "Jaccard similarity",
      "Vector similarity"
    ],
    correct: 2,
    why: "Jaccard ignores frequency and treats both as the same set {apple}."
  },
  {
    question: "Why is edit distance unsuitable for semantic comparison?",
    options: [
      "It is too slow",
      "It depends on string length",
      "It ignores meaning and focuses on characters",
      "It requires normalization"
    ],
    correct: 2,
    why: "Edit distance measures literal character changes, not meaning."
  },
  {
    question: "Which rule best summarizes choosing a similarity measure?",
    options: [
      "Always use cosine similarity",
      "Always use edit distance",
      "Match the measure to the data and task",
      "Use multiple measures together"
    ],
    correct: 2,
    why: "The correct similarity measure depends on what kind of similarity the task requires."
  },
  {
    question: "What is the key learning outcome of this lecture?",
    options: [
      "Similarity is subjective",
      "One measure fits all problems",
      "Different similarity measures serve different purposes",
      "Distance is always numerical"
    ],
    correct: 2,
    why: "The lecture emphasizes choosing the right similarity measure based on the problem and data type."
  }






];


/* ===== PREP + SHUFFLE ===== */
let questions = rawQuestions.map(q => {
    const opts = q.options.map((t,i)=>({t, c:i===q.correct}));
    for(let i=opts.length-1;i>0;i--){
        const j=Math.floor(Math.random()*(i+1));
        [opts[i],opts[j]]=[opts[j],opts[i]];
    }
    return {
        question: q.question,
        options: opts.map(o => o.t),
        correct: opts.findIndex(o => o.c),
        why: q.why
    };

});

let currentQuestion = 0;
let userAnswers = Array(questions.length).fill(null);
let testSubmitted = false;

/* ===== RENDER ===== */
function renderAll(){
    renderQuestion();
    renderNavigator();
    updateProgress();
}

function renderQuestion(){
    const q = questions[currentQuestion];
    let html = `<div class="question-card">
        <div class="question-number">Question ${currentQuestion+1} of ${questions.length}</div>
        <div class="question-text">${q.question}</div>
        <div class="options">`;

    q.options.forEach((opt,i)=>{
        let cls = "";
        if(userAnswers[currentQuestion] === i) cls += " selected";
        if(testSubmitted){
            if(i === q.correct) cls = " correct";
            else if(userAnswers[currentQuestion] === i) cls = " incorrect";
        }
        html += `<div class="option${cls}" onclick="selectOption(${i})">
            <div class="option-label">${String.fromCharCode(65+i)}</div>
            <div>${opt}</div>
        </div>`;
    });

    html += `</div></div>`;
    // Explanation (after submit)
    if (testSubmitted) {
        const userAnswer = userAnswers[currentQuestion];
        const isCorrect = userAnswer === q.correct;

        const explanationClass = isCorrect
            ? "explanation-correct"
            : "explanation-wrong";

        html += `
            <div class="explanation-box ${explanationClass}">
                <div class="explanation-title">
                    ${isCorrect ? "Why this is correct" : "Concept explanation"}
                </div>
                <div>${q.why}</div>
            </div>
        `;
    }

    document.getElementById("quizContainer").innerHTML = html;
}

function renderNavigator(){
    const nav = document.getElementById("questionNavigator");
    nav.innerHTML = "";
    questions.forEach((_,i)=>{
        const b = document.createElement("div");
        b.className = "q-nav-item";
        if(userAnswers[i] !== null) b.classList.add("answered");
        if(i === currentQuestion) b.classList.add("current");
        b.textContent = i+1;
        b.onclick = ()=>{ currentQuestion=i; renderAll(); };
        nav.appendChild(b);
    });
}

function updateProgress(){
    const answered = userAnswers.filter(a=>a!==null).length;
    const pct = Math.round((answered / questions.length) * 100);
    const bar = document.getElementById("progressBar");
    bar.style.width = pct + "%";
    bar.textContent = `${answered}/${questions.length}`;
}

/* ===== ACTIONS ===== */
function selectOption(i){
    if(testSubmitted) return;
    userAnswers[currentQuestion] = i;
    renderAll();
}
function nextQuestion(){
    if(currentQuestion < questions.length-1){
        currentQuestion++;
        renderAll();
    }
}
function previousQuestion(){
    if(currentQuestion > 0){
        currentQuestion--;
        renderAll();
    }
}

function submitTest(){
    if(!confirm("Submit test?")) return;

    testSubmitted = true;

    let correct = 0;
    userAnswers.forEach((a,i)=>{ if(a === questions[i].correct) correct++; });
    const unanswered = userAnswers.filter(a=>a===null).length;
    const percent = ((correct/questions.length)*100).toFixed(1);

    // SHOW OLD RESULT MODAL
    const modal = document.getElementById("results");
    const box = document.getElementById("resultsBox");
    box.innerHTML = `
        <h2>Test Results</h2>
        <div class="score-display">${percent}%</div>
        <div class="stats">
            <div class="stat-card"><div class="stat-value">${correct}</div><div class="stat-label">Correct</div></div>
            <div class="stat-card"><div class="stat-value">${questions.length-correct}</div><div class="stat-label">Incorrect</div></div>
            <div class="stat-card"><div class="stat-value">${unanswered}</div><div class="stat-label">Unanswered</div></div>
            <div class="stat-card"><div class="stat-value">${questions.length}</div><div class="stat-label">Total</div></div>
        </div>`;
    modal.style.display = "flex";

    setTimeout(()=>{
        modal.style.display = "none";

        // REVIEW MODE STRIP
        const strip = document.getElementById("resultStrip");
        strip.innerHTML = `
            <span>Score: ${percent}%</span>
            <span>Correct: ${correct}</span>
            <span>Incorrect: ${questions.length-correct}</span>
            <span>Unanswered: ${unanswered}</span>
        `;
        strip.style.display = "block";
        document.getElementById("retakeBtn").style.display = "inline-block";

        renderAll();
    }, 3000);
}

/* INIT */
renderAll();
</script>

<script>
if ('serviceWorker' in navigator) {
  navigator.serviceWorker.register('./service-worker.js');
}
</script>
<script>
if ('serviceWorker' in navigator) {
  navigator.serviceWorker.register('./service-worker.js');
}
</script>


</body>
</html>

